{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"> Unsupervised Learning Capstone - Data Cleaning and Processing Only Notebook</h3> \n",
    "\n",
    "__Contents__<a name=\"top\"></a>\n",
    "1. [Dataset  Description and Notes](#describe)\n",
    "1. [Objective](#object)\n",
    "1. [Example XML Files](#xml)\n",
    "2. [Cleaning and Processing Data](#clean)\n",
    " - [Process01 Function](#proc1)\n",
    " - [Process02 Function](#proc2)\n",
    " - [Process03 Function](#proc3)\n",
    "3. [Create Dataframe and Save to File](#file)\n",
    "4. [Unit Tests](#test)\n",
    "5. [Scratch](#scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  Description and Notes <a name=\"describe\"></a>\n",
    "\n",
    "Here is the\n",
    "The main reason that I choose this dataset is beyond the text provided by the blog posts, there are four additonal features of gender, age, occupation and sign that can be targeted. I will admit that maybe the idea of using a horoscope sign feature for a target intrigued me more than it should.  \n",
    "\n",
    "The dataset includes:\n",
    "\n",
    "- There are 19,322 xml files that vary greatly in size. Some lack the values for the occupation feature.  While most of the xml values required cleaning before they could be parsed, some had to be discarded because they were no where near *\"well formed\"*.\n",
    "- To develop a classifier model that targets blogger, I used the 10 largest files.\n",
    "- Seperate classifer models that target the other four features are developed from  a set of 200 files with a size of around 25 kb.  That seem to give me a reasonable amount of post samples for each blogger.  I did not use a file that lacked an occupation value nor any with an age of 21 or less.   \n",
    "- Sort xml files by size and select about 200 files in the 25k byte range that have a reasonable amount of text / blog entries\n",
    "-  Filter out files that don't have info for all features.  Filter out bloggers with age less than 25. \n",
    "-  *import lxml.etree as ET* not xml.etree due to decoding issues in the xml files\n",
    "- Some xml files still have decoding issues and are not used. \n",
    "\n",
    "*Please cite the following if you use the data:*\n",
    "\n",
    "J. Schler, M. Koppel, S. Argamon and J. Pennebaker (2006). Effects of Age and Gender on Blogging in Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective<a name=\"object\"></a>\n",
    "\n",
    "Create three sample sets as DataFrames:\n",
    " - \"A\" is from the 10 of the most prolific bloggers with approxiamately 8500 samples.\n",
    " - \"B\" is from approxiamately 200 bloggers  with approxiamately 4500 samples.\n",
    "Create a Dataframe for exploring features and feature engineering\n",
    " - \"F\" is \"B\" without the NLP features and one sample per blogger.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example XML File<a name=\"xml\"></a>\n",
    "\n",
    "The data is contained in xml files and the file name itself. An example file name is: \n",
    "- 3025353.male.35.Religion.Aquarius.xml\n",
    "  - name: 3025353\n",
    "  - gender : male\n",
    "  - age : 35\n",
    "  - occupation : religon\n",
    "  - sign: Aquarius\n",
    "  \n",
    "The root element of the xml file is \"blog\" with child element pairs of \"date\" and \"post\" as shown below:\n",
    "\n",
    "\\< blog \\>\n",
    "- \\< date \\> 30,June,2003 \\< /date \\>\n",
    "- \\< post \\> Anti-war or Civil War?   When I went on  ... It's so hard to make a statement these days. \\< /post \\>\n",
    "- \\< date \\> 24,June,2003 \\< /date \\>\n",
    "- \\< post \\> \"People talk about the 'divorce epidemic'  as if  ...  community that shoots its wounded.\\< /post \\>\n",
    "\n",
    "\\< blog \\>\n",
    "\n",
    "The raw dataset provides seven features or sources of features including \"blogger\", \"gender\", \"age\", \"occupation\", \"sign\", \"date\" and \"post\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Processing Data<a name=\"clean\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Round of Processing<a name=\"proc1\"></a>\n",
    "- Extract Features from File Name\n",
    "- Select Files Based upon Features\n",
    "- Clean xml Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from string import ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files are list of xml files\n",
    "df = pd.read_csv('blogs/files_A.txt', header=0)\n",
    "a_xmls = df.xml.tolist()\n",
    "df = pd.read_csv('blogs/files_B.txt', header=0)\n",
    "b_xmls = df.xml.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree \n",
    "# list of chars that cause XMLSyntaxError during parsing\n",
    "bads = []\n",
    "for i in range(0, 100):\n",
    "    try: x = etree.fromstring(('<p>%s</p>' % chr(i)))\n",
    "    except etree.XMLSyntaxError: bads.append(i)\n",
    "bads.remove(38); bads.remove(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out ages < 21 and with unknow occupation, cleans xml file prior to parsing\n",
    "def process_01(file):\n",
    "    S = file.split(sep='.')\n",
    "    if any([int(S[2]) <= 21, S[3] == 'indUnk']):\n",
    "        return None\n",
    "    else:\n",
    "        txt = Path(r'blogs/' + file).read_text(errors='replace')\n",
    "        clean = ''.join([' ' if ord(t) in bads else t for t in txt])\n",
    "        return {'blgr':S[0], 'gndr':S[1], 'age':int(S[2]), 'ocpn':S[3], 'sign':S[4], 'txt': clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_dcts = [process_01(xml) for xml in a_xmls]\n",
    "b1_dcts = [x for x in [process_01(xml) for xml in b_xmls] if bool(x)]\n",
    "# change blogger id to char becuase as int it causes trouble \n",
    "for (i,dct) in enumerate(a1_dcts):\n",
    "    dct['blgr'] =  ascii_uppercase[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Top__](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Round of Processing<a name=\"proc2\"></a>\n",
    "- Parse xml\n",
    "- Clean Dates\n",
    "- Parse Dates\n",
    "- Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(date):\n",
    "    bad_mth = {'Juni':'June', 'Juli':'July', 'juillet':'July'} \n",
    "    S = date.split(sep=',')\n",
    "    if S[1] in bad_mth:\n",
    "        date = ''.join([S[0], bad_mth[S[1]], S[2]])\n",
    "    return pd.Timestamp(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the xml and filter files lacking dates\n",
    "def process_02(dct, nlp):\n",
    "    parser = etree.XMLParser(remove_blank_text=True, recover=True)\n",
    "    root = etree.fromstring(dct['txt'], parser=parser)\n",
    "    if len(root[0].text) <= 6:\n",
    "        return None\n",
    "    else: \n",
    "        dct['posts']  = [x.text for x in root if x.tag == 'post']\n",
    "        dct['dates'] = [clean_date(x.text) for x in root if x.tag == 'date']\n",
    "    \n",
    "        return dct   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_dcts = [dct for dct in [process_02(dct, nlp) for dct in a1_dcts] if dct != None]\n",
    "b2_dcts = [dct for dct in [process_02(dct, nlp) for dct in b1_dcts] if dct != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Top__](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third Round of Processing<a name=\"proc3\"></a>\n",
    "- Create DataFrame of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform post lemmas to one string and create DataFrame of samples\n",
    "def process_03(dct, name):\n",
    "    n = len(dct['posts'])\n",
    "    lnths = [doc.__len__() for doc in dct['posts']]\n",
    "    if name == 'A':\n",
    "        blgrs = np.repeat(dct['blgr'],n)\n",
    "        df = pd.DataFrame([blgrs, dct['dates'], dct['posts'], lnths], index=['blgr', 'date', 'post', 'lnth']).T    \n",
    "    else:    \n",
    "        gndr = np.repeat(dct['gndr'],n)\n",
    "        age  = np.repeat(dct['age' ],n)\n",
    "        ocpn = np.repeat(dct['ocpn'],n)\n",
    "        sign = np.repeat(dct['sign'],n)\n",
    "        df = pd.DataFrame([ gndr, age, ocpn, sign, dct['dates'], dct['posts'], lnths],\n",
    "                          index=[ 'gndr', 'age', 'ocpn', 'sign', 'date', 'post', 'lnth']).T \n",
    "    #catches blank / mis-parsed posts in xml file\n",
    "    df = df.drop(list(np.where(df.lnth < 3)[0]), axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3_dfs = [process_03(dct, 'A') for dct in a2_dcts]\n",
    "b3_dfs = [process_03(dct, 'B') for dct in b2_dcts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame for blogger \"gender\", \"age\", \"occupation\" and  \"sign\" features.\n",
    "gndrs = pd.Series([dct['gndr']  for dct in b1_dcts])\n",
    "ages  = pd.Series([dct['age']   for dct in b1_dcts])\n",
    "signs = pd.Series([dct['sign']  for dct in b1_dcts]) \n",
    "ocpns = pd.Series([dct['ocpn']  for dct in b1_dcts])\n",
    "dff = pd.DataFrame([gndrs,ages, signs, ocpns], index=['gndrs', 'ages', 'signs', 'ocpns']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Top__](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes and Save to Files<a name=\"file\"></a>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine sub DataFrames and reindex\n",
    "dfa = pd.concat(a3_dfs, axis=0)\n",
    "dfa.index = range(len(dfa))\n",
    "dfb = pd.concat(b3_dfs, axis=0)\n",
    "dfb.index = range(len(dfb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "dfa.to_csv(r'data/dfa.csv')\n",
    "dfb.to_csv(r'data/dfb.csv')\n",
    "dff.to_csv(r'data/dff.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Top__](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests<a name=\"test\"></a>\n",
    "- always be testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading xml file list tests\n",
    "assert len(a_xmls) == 10 == len(df)\n",
    "assert len(b_xmls) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_01 tests\n",
    "t1 = '4326228.male.17.Student.Cancer.xml'\n",
    "t2 = '2278942.male.26.Technology.Virgo.xml'\n",
    "assert process_01(t1) == None\n",
    "assert process_01(t2)['sign'] == 'Virgo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_02 tests\n",
    "assert clean_date('02,June,2004') == pd.Timestamp('06-02-2004')\n",
    "assert clean_date('02,Juli,2004') == pd.Timestamp('07-02-2004')\n",
    "assert b2_dcts[0].keys() == a2_dcts[0].keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch\n",
    "- snippets not used but not ready to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize, lemmaize, use both spaCy and ntlk stops\n",
    "def lemma_post(doc):\n",
    "    stops = stopwords.words('english') + ['urllink', 'urlLink']\n",
    "    A = [d for d in doc if all([d.is_alpha, not d.is_stop])]\n",
    "    B = [a for a in A if a.lemma_ not in stops]\n",
    "    return [b.lemma_ if all([b.pos_ == 'PROPN', not b.is_upper]) else b.lemma_.lower() for b in B] \n",
    "\n",
    "assert lemma_post(nlp('urlLink www.dmbirc.org')) == []\n",
    "assert lemma_post(nlp('Tom is FAT')) == ['Tom', 'fat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the xml and filter files lacking dates\n",
    "def process_02(dct, nlp):\n",
    "    parser = etree.XMLParser(remove_blank_text=True, recover=True)\n",
    "    root = etree.fromstring(dct['txt'], parser=parser)\n",
    "    if len(root[0].text) <= 6:\n",
    "        return None\n",
    "    else: \n",
    "        docs = [nlp(x.text) for x in root if x.tag == 'post']\n",
    "        dct['vocab']  = [doc.vocab for doc in docs]\n",
    "        dct['sents']  = [doc.sents for doc in docs]\n",
    "        dct['lemmas'] = [lemma_post(doc) for doc in docs]\n",
    "        #dct['feels']  = [doc.sentiment for doc in docs]\n",
    "        dct['lngths'] = [len(x.text) for x in root if x.tag == 'post']\n",
    "        dct['dates'] = [clean_date(x.text) for x in root if x.tag == 'date']\n",
    "    \n",
    "        return dct     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
